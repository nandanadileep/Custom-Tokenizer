# -*- coding: utf-8 -*-
"""CustomTokeniser.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Wzya0bduexZTFUQZUjRm6CvRtwbaLfyH
"""

samples = [
    "def foo(x): return x**2",
    "get_user_authentication_token",
    "    indented_code",
    "Ï€ = 3.14159",
    "emoji ðŸ˜… test"
]

for s in samples:
    encoded = s.encode("utf-8")
    decoded = encoded.decode("utf-8")
    print("TEXT:", repr(s))
    print("BYTES:", list(encoded))
    print("DECODED:", repr(decoded))
    print("-" * 40)

"""Step 2: Byte to Symbol Mapping

"""

def bytes_to_symbols(byte_list):
    return [f"<{b}>" for b in byte_list]


def symbols_to_bytes(symbol_list):
    return [int(s[1:-1]) for s in symbol_list]

samples = [
    "def foo(x): return x**2",
    "get_user_authentication_token",
    "    indented_code",
    "Ï€ = 3.14159",
    "emoji ðŸ˜… test"
]

for s in samples:
    byte_values = list(s.encode("utf-8"))
    symbols = bytes_to_symbols(byte_values)
    recovered_bytes = symbols_to_bytes(symbols)
    recovered_text = bytes(recovered_bytes).decode("utf-8")

    print("TEXT:", repr(s))
    print("BYTES:", byte_values)
    print("SYMBOLS:", symbols[:10], "..." if len(symbols) > 10 else "")
    print("RECOVERED:", repr(recovered_text))
    print("MATCH:", s == recovered_text)
    print("-" * 50)

"""Step 3: Learning Merge Rules (BPE)

"""

from collections import Counter

def count_pairs(symbols):
    pairs = Counter()
    for i in range(len(symbols) - 1):
        pairs[(symbols[i], symbols[i+1])] += 1
    return pairs

def merge_pair(symbols, pair, new_symbol):
    merged = []
    i = 0
    while i < len(symbols):
        if i < len(symbols) - 1 and (symbols[i], symbols[i+1]) == pair:
            merged.append(new_symbol)
            i += 2
        else:
            merged.append(symbols[i])
            i += 1
    return merged

def learn_bpe(symbols, num_merges):
    symbols = symbols.copy()
    merges = []

    for _ in range(num_merges):
        pairs = count_pairs(symbols)
        if not pairs:
            break

        best_pair = max(pairs, key=pairs.get)
        new_symbol = best_pair[0] + best_pair[1]

        symbols = merge_pair(symbols, best_pair, new_symbol)
        merges.append((best_pair, new_symbol))

    return symbols, merges

def encode(text, merges):
    bytes_ = list(text.encode("utf-8"))

    symbols = bytes_to_symbols(bytes_)

    for pair, new_symbol in merges:
        symbols = merge_pair(symbols, pair, new_symbol)

    return symbols

new_code = """
def get_user_token(u):
    return u.auth.token
"""

encoded = encode(new_code, merges)

print("Encoded token count:", len(encoded))
print("First 20 tokens:", encoded[:20])

def build_reverse_merges(merges):
    reverse = {}
    for pair, merged in merges.items():
        reverse[merged] = pair
    return reverse

def expand_symbol(symbol, reverse_merges):
    if symbol not in reverse_merges:
        return [symbol]

    a, b = reverse_merges[symbol]
    return expand_symbol(a, reverse_merges) + expand_symbol(b, reverse_merges)

def decode(tokens, merges):
    reverse_merges = build_reverse_merges(merges)

    base_symbols = []
    for token in tokens:
        base_symbols.extend(expand_symbol(token, reverse_merges))

    bytes_ = symbols_to_bytes(base_symbols)

    return bytes(bytes_).decode("utf-8")

tests = [
    "def foo(x): return x**2",
    "get_user_authentication_token",
    "    indented_code",
    "Ï€ = 3.14159",
    "emoji ðŸ˜… test",
    """
def get_user_token(u):
    return u.auth.token
"""
]

for t in tests:
    encoded = encode(t, merges)
    decoded = decode(encoded, merges)

    print("ORIGINAL:", repr(t))
    print("DECODED :", repr(decoded))
    print("MATCH   :", t == decoded)
    print("-" * 60)

